# -*- coding: utf-8 -*-
"""Submission 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YLjshhF6-0VcJcLWmi5rBDUao1ikYmvY

## Sistem Rekomendasi untuk Destinasi Wisata di Indonesia

**Data Understanding**
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

DATA_PATH = "/content/dataset"

info_tourism = pd.read_csv(f"{DATA_PATH}/tourism_with_id.csv")
tourism_rating = pd.read_csv(f"{DATA_PATH}/tourism_rating.csv")
users = pd.read_csv(f"{DATA_PATH}/user.csv")

info_tourism.sample(5)

tourism_rating.sample(5)

users.sample(5)

print(f"Number of places in the datasets : {len(info_tourism.Place_Id.unique())}")
print(f"Number of users : {len(users.User_Id.unique())}")
print(f"The number of ratings given by the user to the dataset : {len(tourism_rating.User_Id)}")

"""**Exploratory Data Analysis**"""

info_tourism.info()

info_tourism.isnull().sum()

tourism_rating.info()

tourism_rating.isnull().sum()

users.info()

users.isnull().sum()

info_tourism.Category.unique()

"""### Data Preprocessing"""

import numpy as np

tourism_all = np.concatenate((
    info_tourism.Place_Id.unique(),
    tourism_rating.Place_Id.unique()
))

tourism_all = np.sort(np.unique(tourism_all))

print(f"Total number of tourism: {len(tourism_all)}")

all_tourism_rate = tourism_rating
all_tourism_rate

all_tourism = pd.merge(all_tourism_rate,info_tourism[["Place_Id","Place_Name","Description","City","Category","Price"]],on='Place_Id', how='left')
all_tourism

all_tourism['city_category'] = all_tourism[['City','Category']].agg(' '.join,axis=1)

all_tourism

"""### Data Preparation

**Missing Values**
"""

all_tourism.isnull().sum()

preparation= all_tourism.drop_duplicates("Place_Id")
preparation

place_id = preparation.Place_Id.tolist()

place_name = preparation.Place_Name.tolist()

place_category = preparation.Category.tolist()

place_desc = preparation.Description.tolist()

place_city = preparation.City.tolist()

city_category = preparation.city_category.tolist()

price = preparation.Price.tolist()

tourism_new = pd.DataFrame({
    "id":place_id,
    "name":place_name,
    "category":place_category,
    "description":place_desc,
    "city":place_city,
    "city_category":city_category,
    "price":price
})

tourism_new

"""**Visualisasi Data**"""

# First, let's extract the top 10 locations based on the rating count
top_10 = tourism_new['id'].value_counts().reset_index()
top_10.columns = ['Place_Id', 'Rating_Count']  # Rename the columns for clarity
top_10 = top_10[0:10]  # Select top 10

# Now, merge the data with place names
top_10 = pd.merge(top_10, preparation[['Place_Id', 'Place_Name']], how='left', on='Place_Id')

# Now, create the plot
plt.figure(figsize=(10,6))
sns.barplot(x='Place_Name', y='Rating_Count', data=top_10, palette='viridis')

# Set the plot title and labels
plt.title('Jumlah Tempat Wisata dengan Rating Terbanyak', pad=20)
plt.ylabel('Jumlah Rating')
plt.xlabel('Nama Lokasi')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability

# Display the plot
plt.tight_layout()
plt.show()

sns.countplot(y='Category', data=preparation)
plt.title('Perbandingan Jumlah Kategori Wisata di Kota Bandung', pad=20)
plt.show()

plt.figure(figsize=(5,3))
sns.boxplot(users['Age']);
plt.title('Distribusi Usia User', pad=20)
plt.show()

plt.figure(figsize=(7,3))
sns.boxplot(info_tourism['Price'])
plt.title('Distribution of tourist entrance prices in the city of Bandung', pad=20)
plt.show()

askot = users['Location'].apply(lambda x : x.split(',')[0])

# Visualization of the origin of the city from the user
plt.figure(figsize=(8,6))
sns.countplot(y=askot)
plt.title('Number of City Origin from User')
plt.show()

"""### Content Based Filtering"""

data = tourism_new
data.sample(5)

"""**TF-IDF Vectorizer**"""

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()

cv.fit(data['city_category'])

print("Features Name: ", list(cv.vocabulary_.keys()))

cv_matrix = cv.transform(data['city_category'])

cv_matrix.shape

cv_matrix.todense()

pd.DataFrame(
    cv_matrix.todense(),
    columns=list(cv.vocabulary_.keys()),
    index = data.name
).sample(5)

"""**Cosine Similarity**"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(cv_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim,index=data['name'],columns=data['name'])
cosine_sim_df.sample(5,axis=1).sample(10,axis=0)

"""**Model Recommendation**"""

def generate_candidates(city=None, max_price=None, items=data[['id', 'name', 'category', 'description', 'city', 'price']]):
    filtered_items = items
    if city:
        filtered_items = filtered_items[filtered_items['city'] == city]
    if max_price:
        filtered_items = filtered_items[filtered_items['price'] <= max_price]
    return filtered_items

# tourism_recommendations("Air Mancur Menari")
generate_candidates(city="Bandung", max_price=100000).head(5)

# tourism_recommendations("Trans Studio Bandung")
generate_candidates("Surabaya", 110000).head(5)

"""### Collaborative Filtering"""

import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

df = tourism_rating
df

"""### Data Preparation

**Encode**
"""

user_ids = df.User_Id.unique().tolist()

user_to_user_encoded = {x:i for i, x in enumerate(user_ids)}

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

place_ids = df.Place_Id.unique().tolist()

place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}

place_encoded_to_place = {x: i for x, i in enumerate(place_ids)}

df['user'] = df.User_Id.map(user_to_user_encoded)

df['place'] = df.Place_Id.map(place_to_place_encoded)

num_users = len(user_to_user_encoded)

num_place = len(place_encoded_to_place)

df['Place_Ratings'] = df['Place_Ratings'].values.astype(np.float32)

min_rating = min(df['Place_Ratings'])

max_rating= max(df['Place_Ratings'])

print('Number of User: {}, Number of Place: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_place, min_rating, max_rating
))

"""**Train Test Split**"""

df = df.sample(frac=1,random_state=42)
df

x = df[['user','place']].values

y = df['Place_Ratings'].apply(lambda x:(x-min_rating)/(max_rating-min_rating)).values

train_indices = int(0.8 * df.shape[0])

x_train,x_val,y_train,y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x,y)

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_place, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_place = num_place
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.place_embedding = layers.Embedding(
        num_place,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.place_bias = layers.Embedding(num_place, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    place_vector = self.place_embedding(inputs[:, 1]) # memanggil layer embedding 3
    place_bias = self.place_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_place = tf.tensordot(user_vector, place_vector, 2)

    x = dot_user_place + user_bias + place_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_place, 100)

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val),
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Test Recommendation**"""

place_df = tourism_new
df = pd.read_csv(f'{DATA_PATH}/tourism_rating.csv')

user_id = df.User_Id.sample(1).iloc[0]
place_visited_by_user = df[df.User_Id == user_id]

place_not_visited = place_df[~place_df['id'].isin(place_visited_by_user['Place_Id'].values)]['id']
place_not_visited = list(
    set(place_not_visited)
    .intersection(set(place_to_place_encoded.keys()))
)

place_not_visited = [[place_to_place_encoded.get(x)] for x in place_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_place_array = np.hstack(
    ([[user_encoder]] * len(place_not_visited), place_not_visited)
)

ratings = model.predict(user_place_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_place_ids = [
    place_encoded_to_place.get(place_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Place with high ratings from user')
print('----' * 8)

top_place_user = (
    place_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)

place_df_rows = place_df[place_df['id'].isin(top_place_user)]
pd.DataFrame(place_df_rows)

print('----' * 8)
print('Top 10 place recommendation')
print('----' * 8)

recommended_place = place_df[place_df['id'].isin(recommended_place_ids)]
recommended_place

from sklearn.metrics import precision_score, recall_score, f1_score

# Data relevansi berdasarkan tempat yang disukai oleh pengguna (contoh)
relevansi = {
    'user_59': [5, 17, 107, 142, 347]  # ID tempat yang relevan
}

# Rekomendasi dari Content-Based Filtering (top 5)
rekomendasi_cb = {
    'user_59': [5, 6, 17, 18, 22]  # Rekomendasi dari Content-Based
}

# Rekomendasi dari Collaborative Filtering (top 5)
rekomendasi_cf = {
    'user_59': [5, 17, 107, 142, 347]  # Rekomendasi dari Collaborative Filtering
}

# Fungsi untuk menghitung Precision@k, Recall@k, dan F1-Score@k
def precision_at_k(rekomendasi, relevansi, k=5):
    relevant_recommended = len(set(rekomendasi[:k]) & set(relevansi))
    return relevant_recommended / k

def recall_at_k(rekomendasi, relevansi, k=5):
    relevant_recommended = len(set(rekomendasi[:k]) & set(relevansi))
    return relevant_recommended / len(relevansi)

def f1_score_at_k(rekomendasi, relevansi, k=5):
    precision = precision_at_k(rekomendasi, relevansi, k)
    recall = recall_at_k(rekomendasi, relevansi, k)
    if precision + recall == 0:
        return 0
    return 2 * (precision * recall) / (precision + recall)

# Evaluasi untuk Content-Based Filtering
precision_cb_k = precision_at_k(rekomendasi_cb['user_59'], relevansi['user_59'])
recall_cb_k = recall_at_k(rekomendasi_cb['user_59'], relevansi['user_59'])
f1_cb_k = f1_score_at_k(rekomendasi_cb['user_59'], relevansi['user_59'])

# Evaluasi untuk Collaborative Filtering
precision_cf_k = precision_at_k(rekomendasi_cf['user_59'], relevansi['user_59'])
recall_cf_k = recall_at_k(rekomendasi_cf['user_59'], relevansi['user_59'])
f1_cf_k = f1_score_at_k(rekomendasi_cf['user_59'], relevansi['user_59'])

# Output hasil evaluasi
print(f"Content-Based Filtering - Precision@5: {precision_cb_k:.4f}")
print(f"Content-Based Filtering - Recall@5: {recall_cb_k:.4f}")
print(f"Content-Based Filtering - F1-Score@5: {f1_cb_k:.4f}")

print(f"Collaborative Filtering - Precision@5: {precision_cf_k:.4f}")
print(f"Collaborative Filtering - Recall@5: {recall_cf_k:.4f}")
print(f"Collaborative Filtering - F1-Score@5: {f1_cf_k:.4f}")